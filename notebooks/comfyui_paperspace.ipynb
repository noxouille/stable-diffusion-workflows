{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6683b981-7659-4316-9a4b-f20663deffba",
   "metadata": {},
   "source": [
    "# Setup workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3768ed9-81d6-49f1-9cfa-8dacdb403baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# Setup venv\n",
    "# !sudo apt install -y python3.10-venv\n",
    "# !python3 -m venv sd\n",
    "# !source sd/bin/activate\n",
    "\n",
    "# Clone repo\n",
    "WORKSPACE = 'ComfyUI'\n",
    "![ ! -d $WORKSPACE ] && echo -= Initial setup ComfyUI =- && git clone https://github.com/comfyanonymous/ComfyUI\n",
    "%cd $WORKSPACE\n",
    "\n",
    "!echo -= Updating ComfyUI =-\n",
    "!git pull\n",
    "\n",
    "# Directories\n",
    "models_dir = \"./models/checkpoints/\"\n",
    "clips_dir = \"./models/clip/\"\n",
    "clip_vision_dir = \"./models/clip_vision/\"\n",
    "configs_dir = \"./models/configs/\"\n",
    "controlnet_dir = \"./models/controlnet/\"\n",
    "diffusers_dir = \"./models/diffusers/\"\n",
    "embeddings_dir = \"./models/embeddings/\"\n",
    "gligen_dir = \"./models/gligen/\"\n",
    "hypernetworks_dir = \"./models/hypernetworks/\"\n",
    "loras_dir = \"./models/loras/\"\n",
    "style_models_dir = \"./models/style_models/\"\n",
    "unet_dir = \"./models/unet/\"\n",
    "upscale_models_dir = \"./models/upscale_models/\"\n",
    "vae_dir = \"./models/vae/\"\n",
    "vae_approx_dir = \"./models/vae_approx/\"\n",
    "custom_nodes_dir = \"./custom_nodes/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8aa398-c431-429d-871e-0526da360999",
   "metadata": {},
   "source": [
    "## Uninstall previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c7f5f-f606-4e34-8e41-888f086f0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y torch torchvision torchaudio torchsde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e49da7e-9d08-4df2-ba6a-5a9a8055507e",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b06a7-3b40-4879-a77b-4518ac9c2f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !echo -= Install dependencies =-\n",
    "# !pip install xformers!=0.0.18 torchvision -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "# !sudo apt install -y aria2\n",
    "\n",
    "optional_huggingface_token = \"\" #@param{type:\"string\"}\n",
    "token = optional_huggingface_token if optional_huggingface_token else \"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\"\n",
    "user_header = f\"\\\"Authorization: Bearer {token}\\\"\"\n",
    "\n",
    "def download(urls, dst):\n",
    "  for url in urls:\n",
    "    url = url.strip()\n",
    "    if not url:\n",
    "      continue\n",
    "    elif 'huggingface' in url:\n",
    "      if '/blob/' in url:\n",
    "        url = url.replace('/blob/', '/resolve/')\n",
    "      parsed_link = '\\n{}\\n\\tout={}'.format(url,unquote(url.split('/')[-1]))\n",
    "      !echo -e \"{parsed_link}\" | aria2c --header={user_header} --console-log-level=error --summary-interval=10 -i- -j5 -x16 -s16 -k1M -c -d \"{dst}\"\n",
    "    else:\n",
    "      !aria2c --optimize-concurrent-downloads --console-log-level=error --summary-interval=10 -j5 -x 16 -s 16 -c -d {dst} {url}\n",
    "  urls = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbf0ed-c7cc-4103-b239-7db2bb4c039b",
   "metadata": {},
   "source": [
    "# SDXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfb50a-3f5b-4519-8203-6d0ac87464a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checkpoints\n",
    "## I recommend these workflow examples: https://comfyanonymous.github.io/ComfyUI_examples/sdxl/\n",
    "sd_xl_base_1_0 = False\n",
    "sd_xl_refiner_1_0 = False\n",
    "DreamShaper_xl_1_0_alpha2 = False\n",
    "ProtoVisionXL_beta_0_5_2_0_bakedVAE = False\n",
    "unCLIP = False\n",
    "\n",
    "models_url = set()\n",
    "if sd_xl_base_1_0:\n",
    "  models_url.add(\"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\")\n",
    "if sd_xl_refiner_1_0:\n",
    "  models_url.add(\"https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0.safetensors\")\n",
    "if DreamShaper_xl_1_0_alpha2:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/126688\")\n",
    "if ProtoVisionXL_beta_0_5_2_0_bakedVAE:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/144229\")\n",
    "if unCLIP:\n",
    "  models_url.add(\"https://huggingface.co/comfyanonymous/illuminatiDiffusionV1_v11_unCLIP/resolve/main/illuminatiDiffusionV1_v11-unclip-h-fp16.safetensors\")\n",
    "  models_url.add(\"https://huggingface.co/comfyanonymous/wd-1.5-beta2_unCLIP/resolve/main/wd-1-5-beta2-aesthetic-unclip-h-fp16.safetensors\")\n",
    "\n",
    "download(models_url, models_dir)\n",
    "\n",
    "# VAE\n",
    "sdxl_vae = False\n",
    "vae_url = set()\n",
    "\n",
    "if sdxl_vae:\n",
    "  vae_url.add(\"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\")\n",
    "\n",
    "download(vae_url, vae_dir)\n",
    "\n",
    "# LoRAs\n",
    "sd_xl_offset_example_lora_1_0 = False\n",
    "loras_url = set()\n",
    "\n",
    "if sd_xl_offset_example_lora_1_0:\n",
    "  loras_url.add(\"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors\") #SDXL offset noise lora\n",
    "\n",
    "download(loras_url, loras_dir)\n",
    "\n",
    "# ControlNet\n",
    "control_LoRAs_rank128 = False\n",
    "control_LoRAs_rank256 = False\n",
    "controlnet_url = set()\n",
    "\n",
    "if control_LoRAs_rank128:\n",
    "  controlnet_url.add(\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank128/control-lora-canny-rank128.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank128/control-lora-depth-rank128.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank128/control-lora-recolor-rank128.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank128/control-lora-sketch-rank128.safetensors\")\n",
    "if control_LoRAs_rank256:\n",
    "  controlnet_url.add(\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-canny-rank256.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-depth-rank256.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-recolor-rank256.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-sketch-rank256.safetensors\")\n",
    "\n",
    "download(controlnet_url, controlnet_dir)\n",
    "\n",
    "# Upscaler\n",
    "NMKD_Siax_200k4x = True\n",
    "UltraSharp4x = True\n",
    "upscaler_url = set()\n",
    "\n",
    "if NMKD_Siax_200k4x:\n",
    "  upscaler_url.add(\"https://huggingface.co/uwg/upscaler/resolve/main/ESRGAN/4x_NMKD-Siax_200k.pth\")\n",
    "if UltraSharp4x:\n",
    "  upscaler_url.add(\"https://huggingface.co/uwg/upscaler/resolve/main/ESRGAN/4x-UltraSharp.pth\")\n",
    "\n",
    "download(upscaler_url, upscale_models_dir)\n",
    "\n",
    "# Custom Nodes\n",
    "!git clone https://github.com/ltdrdata/ComfyUI-Manager.git {custom_nodes_dir}ComfyUI-Manager\n",
    "!git clone https://github.com/LucianoCirino/efficiency-nodes-comfyui.git {custom_nodes_dir}efficiency-nodes-comfyui\n",
    "!git clone https://github.com/ssitu/ComfyUI_NestedNodeBuilder.git {custom_nodes_dir}ComfyUI_NestedNodeBuilder\n",
    "!git clone https://github.com/bmad4ever/ComfyUI-Bmad-DirtyUndoRedo.git {custom_nodes_dir}ComfyUI-Bmad-DirtyUndoRedo\n",
    "!git clone https://github.com/Derfuu/Derfuu_ComfyUI_ModdedNodes {custom_nodes_dir}Derfuu_ComfyUI_ModdedNodes\n",
    "!git clone https://github.com/ltdrdata/ComfyUI-Impact-Pack.git {custom_nodes_dir}ComfyUI-Impact-Pack\n",
    "!git clone https://github.com/evanspearman/ComfyMath.git {custom_nodes_dir}ComfyMath\n",
    "!git clone https://github.com/marhensa/sdxl-recommended-res-calc.git {custom_nodes_dir}sdxl-recommended-res-calc\n",
    "!git clone https://github.com/ssitu/ComfyUI_UltimateSDUpscale {custom_nodes_dir}ComfyUI_UltimateSDUpscale --recursive\n",
    "\n",
    "if controlnet_url:\n",
    "  # Controlnet Preprocessor nodes by Fannovel16\n",
    "  !cd custom_nodes && git clone https://github.com/Fannovel16/comfyui_controlnet_aux.git; cd comfyui_controlnet_aux && python install.py;pip install -r requirements.txt\n",
    "\n",
    "# CLIPVision model (needed for styles model)\n",
    "# By default, download clip_vision_g when sdxl base model is selected (required for ReVision workflow)\n",
    "clipvision_model_url = set()\n",
    "\n",
    "if sd_xl_base_1_0:\n",
    "  clipvision_model_url.add(\"https://huggingface.co/comfyanonymous/clip_vision_g/resolve/main/clip_vision_g.safetensors\")\n",
    "\n",
    "download(clipvision_model_url, clip_vision_dir)\n",
    "\n",
    "# Workflows\n",
    "!git clone https://github.com/SeargeDP/SeargeSDXL.git {custom_nodes_dir}SeargeSDXL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e41797-a601-4be7-b9f9-4a529795ff1b",
   "metadata": {},
   "source": [
    "# SD 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e6e8d-de25-4539-a026-c492c6b7fc2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checkpoints\n",
    "SD1_5 = False\n",
    "dreamshaper_v8 = False\n",
    "NeverEndingDream_v1_22_baked_vae = False\n",
    "Chilloutmix_Ni_pruned_fp32_fix = False\n",
    "PerfectDeliberate_v4_0 = False\n",
    "PerfectDeliberate_anime_v1_0 = False\n",
    "fCAnimeMix_v3_0 = False\n",
    "fCRadianceApex_v2_0 = False\n",
    "fCBlendMix_v2_1 = False\n",
    "RealisticVision_v5_1_VAE = True\n",
    "MeichiDark_v4_5 = True\n",
    "Fantexi_v0_9_beta = False\n",
    "SakushiMix_finalVer = False\n",
    "soapmix28D_v10 = False\n",
    "AbyssOrangeMix2_hard = False\n",
    "AOM3A1_orangemixs = False\n",
    "AOM3A3_orangemixs = False\n",
    "anything_v3_fp16_pruned = False\n",
    "waifu_diffusion_illusion_fp16 = False\n",
    "models_url = set()\n",
    "\n",
    "if SD1_5:\n",
    "  models_url.add(\"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\")\n",
    "if dreamshaper_v8:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/128713\")\n",
    "if NeverEndingDream_v1_22_baked_vae:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/64094\")\n",
    "if Chilloutmix_Ni_pruned_fp32_fix:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/11745\")\n",
    "if PerfectDeliberate_v4_0:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/86698\")\n",
    "if PerfectDeliberate_anime_v1_0:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/120036\")\n",
    "if fCAnimeMix_v3_0:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/103277\")\n",
    "if fCRadianceApex_v2_0:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/91270\")\n",
    "if fCBlendMix_v2_1:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/90444\")\n",
    "if RealisticVision_v5_1_VAE:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/130072\")\n",
    "if MeichiDark_v4_5:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/147184\")\n",
    "if Fantexi_v0_9_beta:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/95199\")\n",
    "if SakushiMix_finalVer:\n",
    "  models_url.add(\"https://civitai.com/api/download/models/133274\")\n",
    "if soapmix28D_v10:\n",
    "  models_url.add(\"https://huggingface.co/jtamph/soapmix/resolve/main/soapmix28D_v10.safetensors\")\n",
    "if AbyssOrangeMix2_hard:\n",
    "  models_url.add(\"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_hard.safetensors\")\n",
    "if AOM3A1_orangemixs:\n",
    "  models_url.add(\"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A1_orangemixs.safetensors\")\n",
    "if AOM3A3_orangemixs:\n",
    "  models_url.add(\"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A3_orangemixs.safetensors\")\n",
    "if anything_v3_fp16_pruned:\n",
    "  models_url.add(\"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-fp16-pruned.safetensors\")\n",
    "if waifu_diffusion_illusion_fp16:  # Waifu Diffusion 1.5 (anime style SD2.x 768-v)\n",
    "  models_url.add(\"https://huggingface.co/waifu-diffusion/wd-1-5-beta3/resolve/main/wd-illusion-fp16.safetensors\")\n",
    "\n",
    "download(models_url, models_dir)\n",
    "\n",
    "# VAE\n",
    "vae_ft_mse_840000_ema_pruned = True\n",
    "orangemix_vae = False\n",
    "kl_f8_anime2 = True\n",
    "vae_url = set()\n",
    "\n",
    "if vae_ft_mse_840000_ema_pruned:\n",
    "  vae_url.add(\"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors\")\n",
    "if orangemix_vae:\n",
    "  vae_url.add(\"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt\")\n",
    "if kl_f8_anime2:\n",
    "  vae_url.add(\"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt\")\n",
    "\n",
    "download(vae_url, vae_dir)\n",
    "\n",
    "# Embeddings\n",
    "EasyNegative = True\n",
    "BadDream_v1_0 = True\n",
    "UnrealisticDream_v1_0 = True\n",
    "FastNegative_v2_0 = True\n",
    "embeddings_url = set()\n",
    "\n",
    "if EasyNegative:\n",
    "  embeddings_url.add(\"https://civitai.com/api/download/models/9208\")\n",
    "if BadDream_v1_0:\n",
    "  embeddings_url.add(\"https://civitai.com/api/download/models/77169\")\n",
    "if UnrealisticDream_v1_0:\n",
    "  embeddings_url.add(\"https://civitai.com/api/download/models/77173\")\n",
    "if FastNegative_v2_0:\n",
    "  embeddings_url.add(\"https://civitai.com/api/download/models/94057\")\n",
    "\n",
    "download(embeddings_url, embeddings_dir)\n",
    "\n",
    "# LoRAs\n",
    "theovercomer8sContrastFix_sd21768 = False\n",
    "theovercomer8sContrastFix_sd15 = False\n",
    "LowRA = False\n",
    "loras_url = set()\n",
    "\n",
    "if theovercomer8sContrastFix_sd21768:\n",
    "  loras_url.add(\"https://civitai.com/api/download/models/10350\") #theovercomer8sContrastFix SD2.x 768-v\n",
    "if theovercomer8sContrastFix_sd15:\n",
    "  loras_url.add(\"https://civitai.com/api/download/models/10638\") #theovercomer8sContrastFix SD1.x\n",
    "if LowRA:\n",
    "  loras_url.add(\"https://civitai.com/api/download/models/63006\")\n",
    "\n",
    "download(loras_url, loras_dir)\n",
    "\n",
    "# ControlNet\n",
    "controlnet_sd_1_5 = False\n",
    "t2i_adapter = False\n",
    "controlnet_url = set()\n",
    "\n",
    "if controlnet_sd_1_5:\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_ip2p_fp16.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_shuffle_fp16.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_canny_fp16.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_inpaint_fp16.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_lineart_fp16.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_mlsd_fp16.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_normalbae_fp16.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_seg_fp16.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_softedge_fp16.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15s2_lineart_anime_fp16.safetensors\")\n",
    "  controlnet_url.add(\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11u_sd15_tile_fp16.safetensors\")\n",
    "if t2i_adapter:\n",
    "  controlnet_url.add(\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd14v1.pth\")\n",
    "  controlnet_url.add(\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_seg_sd14v1.pth\")\n",
    "  controlnet_url.add(\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_sketch_sd14v1.pth\")\n",
    "  controlnet_url.add(\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_keypose_sd14v1.pth\")\n",
    "  controlnet_url.add(\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_openpose_sd14v1.pth\")\n",
    "  controlnet_url.add(\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_color_sd14v1.pth\")\n",
    "  controlnet_url.add(\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_canny_sd14v1.pth\")\n",
    "\n",
    "download(controlnet_url, controlnet_dir)\n",
    "\n",
    "# Upscaler\n",
    "esrgan = False\n",
    "upscaler_url = set()\n",
    "\n",
    "if esrgan:\n",
    "  upscaler_url.add(\"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth\")\n",
    "  upscaler_url.add(\"https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x2.pth\")\n",
    "  upscaler_url.add(\"https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x4.pth\")\n",
    "\n",
    "download(upscaler_url, upscale_models_dir)\n",
    "\n",
    "# Styles Model\n",
    "t2i_styles_model = False\n",
    "styles_model_url = set()\n",
    "\n",
    "if t2i_styles_model:\n",
    "  styles_model_url.add(\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_style_sd14v1.pth\")\n",
    "\n",
    "download(styles_model_url, style_models_dir)\n",
    "\n",
    "# CLIPVision model (needed for styles model)\n",
    "CLIPVision_model = False\n",
    "clipvision_model_url = set()\n",
    "\n",
    "if CLIPVision_model:\n",
    "  clipvision_model_url.add(\"https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin\")\n",
    "\n",
    "download(clipvision_model_url, clip_vision_dir)\n",
    "\n",
    "# GLIGEN\n",
    "gligen_model = False\n",
    "gligen_url = set()\n",
    "\n",
    "if gligen_model:\n",
    "  gligen_url.add(\"https://huggingface.co/comfyanonymous/GLIGEN_pruned_safetensors/resolve/main/gligen_sd14_textbox_pruned_fp16.safetensors\")\n",
    "\n",
    "download(gligen_url, gligen_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8fab87-c785-4a1a-9099-690726e0b6f3",
   "metadata": {},
   "source": [
    "# SD 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92baed51-0f26-40dd-abdb-6a08dc01a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints\n",
    "v2_1_512_ema_pruned = False\n",
    "v2_1_768_ema_pruned = False\n",
    "models_url = set()\n",
    "\n",
    "if v2_1_512_ema_pruned:\n",
    "  models_url.add(\"https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.safetensors\")\n",
    "if v2_1_768_ema_pruned:\n",
    "  models_url.add(\"https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.safetensors\")\n",
    "\n",
    "download(models_url, models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d245349-2f00-4227-9993-950b40052d3f",
   "metadata": {},
   "source": [
    "# Run ComfyUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec6e3ee-013e-4de6-9415-10ea32a3069f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ComfyUI with  cloudflared (Recommended Way)\n",
      "--2023-08-29 03:41:58--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/cloudflare/cloudflared/releases/download/2023.8.1/cloudflared-linux-amd64.deb [following]\n",
      "--2023-08-29 03:41:58--  https://github.com/cloudflare/cloudflared/releases/download/2023.8.1/cloudflared-linux-amd64.deb\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/9b83e911-3f53-4c71-9333-fab1afc4ecbd?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230829%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230829T034125Z&X-Amz-Expires=300&X-Amz-Signature=145912cc3bd88e156140b3bf7ffa5355ede08f59de2529dcccebdf40d4970abd&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=106867604&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64.deb&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-08-29 03:41:58--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/9b83e911-3f53-4c71-9333-fab1afc4ecbd?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230829%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230829T034125Z&X-Amz-Expires=300&X-Amz-Signature=145912cc3bd88e156140b3bf7ffa5355ede08f59de2529dcccebdf40d4970abd&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=106867604&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64.deb&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17701974 (17M) [application/octet-stream]\n",
      "Saving to: ‘cloudflared-linux-amd64.deb.4’\n",
      "\n",
      "cloudflared-linux-a 100%[===================>]  16.88M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2023-08-29 03:41:58 (311 MB/s) - ‘cloudflared-linux-amd64.deb.4’ saved [17701974/17701974]\n",
      "\n",
      "(Reading database ... 70185 files and directories currently installed.)\n",
      "Preparing to unpack cloudflared-linux-amd64.deb ...\n",
      "Unpacking cloudflared (2023.8.1) over (2023.8.1) ...\n",
      "Setting up cloudflared (2023.8.1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "** ComfyUI start up time: 2023-08-29 03:42:01.062505\n",
      "\n",
      "Prestartup times for custom nodes:\n",
      "   0.0 seconds: /notebooks/ComfyUI/custom_nodes/ComfyUI-Manager\n",
      "\n",
      "Total VRAM 16273 MB, total RAM 30068 MB\n",
      "xformers version: 0.0.21\n",
      "Set vram state to: NORMAL_VRAM\n",
      "Device: cuda:0 Quadro P5000 : cudaMallocAsync\n",
      "VAE dtype: torch.float32\n",
      "Using xformers cross attention\n",
      "### Loading: ComfyUI-Impact-Pack (V3.25.2)\n",
      "### Loading: ComfyUI-Impact-Pack (Subpack: V0.2)\n",
      "\u001b[92mBmad-DirtyUndoRedo Loaded.\u001b[0m\n",
      "### Loading: ComfyUI-Manager (V0.26.2)\n",
      "### ComfyUI Revision: 1358 [65cae62c]\n",
      "\n",
      "Import times for custom nodes:\n",
      "   0.0 seconds: /notebooks/ComfyUI/custom_nodes/sdxl-recommended-res-calc\n",
      "   0.0 seconds: /notebooks/ComfyUI/custom_nodes/ComfyUI-Bmad-DirtyUndoRedo\n",
      "   0.0 seconds: /notebooks/ComfyUI/custom_nodes/ComfyUI_NestedNodeBuilder\n",
      "   0.0 seconds: /notebooks/ComfyUI/custom_nodes/ComfyMath\n",
      "   0.0 seconds: /notebooks/ComfyUI/custom_nodes/ComfyUI_UltimateSDUpscale\n",
      "   0.0 seconds: /notebooks/ComfyUI/custom_nodes/SeargeSDXL\n",
      "   0.0 seconds: /notebooks/ComfyUI/custom_nodes/ComfyUI-Manager\n",
      "   0.0 seconds: /notebooks/ComfyUI/custom_nodes/Derfuu_ComfyUI_ModdedNodes\n",
      "   0.7 seconds: /notebooks/ComfyUI/custom_nodes/ComfyUI-Impact-Pack\n",
      "   0.8 seconds: /notebooks/ComfyUI/custom_nodes/efficiency-nodes-comfyui\n",
      "\n",
      "\n",
      "ComfyUI finished loading, trying to launch cloudflared (if it gets stuck here cloudflared is having issues)\n",
      "\n",
      "This is the URL to access ComfyUI: https://diy-statistics-compatible-ld.trycloudflare.com                                    |\n",
      "FETCH DATA from: /notebooks/ComfyUI/custom_nodes/ComfyUI-Manager/extension-node-map.json\n",
      "FETCH DATA from: /notebooks/ComfyUI/custom_nodes/ComfyUI-Manager/extension-node-map.json\n",
      "got prompt\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "model_type EPS\n",
      "adm 0\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "missing {'cond_stage_model.logit_scale', 'cond_stage_model.text_projection'}\n",
      "left over keys: dict_keys(['cond_stage_model.transformer.text_model.embeddings.position_ids', 'control_model.input_blocks.0.0.bias', 'control_model.input_blocks.0.0.weight', 'control_model.input_blocks.1.0.emb_layers.1.bias', 'control_model.input_blocks.1.0.emb_layers.1.weight', 'control_model.input_blocks.1.0.in_layers.0.bias', 'control_model.input_blocks.1.0.in_layers.0.weight', 'control_model.input_blocks.1.0.in_layers.2.bias', 'control_model.input_blocks.1.0.in_layers.2.weight', 'control_model.input_blocks.1.0.out_layers.0.bias', 'control_model.input_blocks.1.0.out_layers.0.weight', 'control_model.input_blocks.1.0.out_layers.3.bias', 'control_model.input_blocks.1.0.out_layers.3.weight', 'control_model.input_blocks.1.1.norm.bias', 'control_model.input_blocks.1.1.norm.weight', 'control_model.input_blocks.1.1.proj_in.bias', 'control_model.input_blocks.1.1.proj_in.weight', 'control_model.input_blocks.1.1.proj_out.bias', 'control_model.input_blocks.1.1.proj_out.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.10.0.emb_layers.1.bias', 'control_model.input_blocks.10.0.emb_layers.1.weight', 'control_model.input_blocks.10.0.in_layers.0.bias', 'control_model.input_blocks.10.0.in_layers.0.weight', 'control_model.input_blocks.10.0.in_layers.2.bias', 'control_model.input_blocks.10.0.in_layers.2.weight', 'control_model.input_blocks.10.0.out_layers.0.bias', 'control_model.input_blocks.10.0.out_layers.0.weight', 'control_model.input_blocks.10.0.out_layers.3.bias', 'control_model.input_blocks.10.0.out_layers.3.weight', 'control_model.input_blocks.11.0.emb_layers.1.bias', 'control_model.input_blocks.11.0.emb_layers.1.weight', 'control_model.input_blocks.11.0.in_layers.0.bias', 'control_model.input_blocks.11.0.in_layers.0.weight', 'control_model.input_blocks.11.0.in_layers.2.bias', 'control_model.input_blocks.11.0.in_layers.2.weight', 'control_model.input_blocks.11.0.out_layers.0.bias', 'control_model.input_blocks.11.0.out_layers.0.weight', 'control_model.input_blocks.11.0.out_layers.3.bias', 'control_model.input_blocks.11.0.out_layers.3.weight', 'control_model.input_blocks.2.0.emb_layers.1.bias', 'control_model.input_blocks.2.0.emb_layers.1.weight', 'control_model.input_blocks.2.0.in_layers.0.bias', 'control_model.input_blocks.2.0.in_layers.0.weight', 'control_model.input_blocks.2.0.in_layers.2.bias', 'control_model.input_blocks.2.0.in_layers.2.weight', 'control_model.input_blocks.2.0.out_layers.0.bias', 'control_model.input_blocks.2.0.out_layers.0.weight', 'control_model.input_blocks.2.0.out_layers.3.bias', 'control_model.input_blocks.2.0.out_layers.3.weight', 'control_model.input_blocks.2.1.norm.bias', 'control_model.input_blocks.2.1.norm.weight', 'control_model.input_blocks.2.1.proj_in.bias', 'control_model.input_blocks.2.1.proj_in.weight', 'control_model.input_blocks.2.1.proj_out.bias', 'control_model.input_blocks.2.1.proj_out.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.3.0.op.bias', 'control_model.input_blocks.3.0.op.weight', 'control_model.input_blocks.4.0.emb_layers.1.bias', 'control_model.input_blocks.4.0.emb_layers.1.weight', 'control_model.input_blocks.4.0.in_layers.0.bias', 'control_model.input_blocks.4.0.in_layers.0.weight', 'control_model.input_blocks.4.0.in_layers.2.bias', 'control_model.input_blocks.4.0.in_layers.2.weight', 'control_model.input_blocks.4.0.out_layers.0.bias', 'control_model.input_blocks.4.0.out_layers.0.weight', 'control_model.input_blocks.4.0.out_layers.3.bias', 'control_model.input_blocks.4.0.out_layers.3.weight', 'control_model.input_blocks.4.0.skip_connection.bias', 'control_model.input_blocks.4.0.skip_connection.weight', 'control_model.input_blocks.4.1.norm.bias', 'control_model.input_blocks.4.1.norm.weight', 'control_model.input_blocks.4.1.proj_in.bias', 'control_model.input_blocks.4.1.proj_in.weight', 'control_model.input_blocks.4.1.proj_out.bias', 'control_model.input_blocks.4.1.proj_out.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.5.0.emb_layers.1.bias', 'control_model.input_blocks.5.0.emb_layers.1.weight', 'control_model.input_blocks.5.0.in_layers.0.bias', 'control_model.input_blocks.5.0.in_layers.0.weight', 'control_model.input_blocks.5.0.in_layers.2.bias', 'control_model.input_blocks.5.0.in_layers.2.weight', 'control_model.input_blocks.5.0.out_layers.0.bias', 'control_model.input_blocks.5.0.out_layers.0.weight', 'control_model.input_blocks.5.0.out_layers.3.bias', 'control_model.input_blocks.5.0.out_layers.3.weight', 'control_model.input_blocks.5.1.norm.bias', 'control_model.input_blocks.5.1.norm.weight', 'control_model.input_blocks.5.1.proj_in.bias', 'control_model.input_blocks.5.1.proj_in.weight', 'control_model.input_blocks.5.1.proj_out.bias', 'control_model.input_blocks.5.1.proj_out.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.6.0.op.bias', 'control_model.input_blocks.6.0.op.weight', 'control_model.input_blocks.7.0.emb_layers.1.bias', 'control_model.input_blocks.7.0.emb_layers.1.weight', 'control_model.input_blocks.7.0.in_layers.0.bias', 'control_model.input_blocks.7.0.in_layers.0.weight', 'control_model.input_blocks.7.0.in_layers.2.bias', 'control_model.input_blocks.7.0.in_layers.2.weight', 'control_model.input_blocks.7.0.out_layers.0.bias', 'control_model.input_blocks.7.0.out_layers.0.weight', 'control_model.input_blocks.7.0.out_layers.3.bias', 'control_model.input_blocks.7.0.out_layers.3.weight', 'control_model.input_blocks.7.0.skip_connection.bias', 'control_model.input_blocks.7.0.skip_connection.weight', 'control_model.input_blocks.7.1.norm.bias', 'control_model.input_blocks.7.1.norm.weight', 'control_model.input_blocks.7.1.proj_in.bias', 'control_model.input_blocks.7.1.proj_in.weight', 'control_model.input_blocks.7.1.proj_out.bias', 'control_model.input_blocks.7.1.proj_out.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.8.0.emb_layers.1.bias', 'control_model.input_blocks.8.0.emb_layers.1.weight', 'control_model.input_blocks.8.0.in_layers.0.bias', 'control_model.input_blocks.8.0.in_layers.0.weight', 'control_model.input_blocks.8.0.in_layers.2.bias', 'control_model.input_blocks.8.0.in_layers.2.weight', 'control_model.input_blocks.8.0.out_layers.0.bias', 'control_model.input_blocks.8.0.out_layers.0.weight', 'control_model.input_blocks.8.0.out_layers.3.bias', 'control_model.input_blocks.8.0.out_layers.3.weight', 'control_model.input_blocks.8.1.norm.bias', 'control_model.input_blocks.8.1.norm.weight', 'control_model.input_blocks.8.1.proj_in.bias', 'control_model.input_blocks.8.1.proj_in.weight', 'control_model.input_blocks.8.1.proj_out.bias', 'control_model.input_blocks.8.1.proj_out.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.9.0.op.bias', 'control_model.input_blocks.9.0.op.weight', 'control_model.input_hint_block.0.bias', 'control_model.input_hint_block.0.weight', 'control_model.input_hint_block.10.bias', 'control_model.input_hint_block.10.weight', 'control_model.input_hint_block.12.bias', 'control_model.input_hint_block.12.weight', 'control_model.input_hint_block.14.bias', 'control_model.input_hint_block.14.weight', 'control_model.input_hint_block.2.bias', 'control_model.input_hint_block.2.weight', 'control_model.input_hint_block.4.bias', 'control_model.input_hint_block.4.weight', 'control_model.input_hint_block.6.bias', 'control_model.input_hint_block.6.weight', 'control_model.input_hint_block.8.bias', 'control_model.input_hint_block.8.weight', 'control_model.middle_block.0.emb_layers.1.bias', 'control_model.middle_block.0.emb_layers.1.weight', 'control_model.middle_block.0.in_layers.0.bias', 'control_model.middle_block.0.in_layers.0.weight', 'control_model.middle_block.0.in_layers.2.bias', 'control_model.middle_block.0.in_layers.2.weight', 'control_model.middle_block.0.out_layers.0.bias', 'control_model.middle_block.0.out_layers.0.weight', 'control_model.middle_block.0.out_layers.3.bias', 'control_model.middle_block.0.out_layers.3.weight', 'control_model.middle_block.1.norm.bias', 'control_model.middle_block.1.norm.weight', 'control_model.middle_block.1.proj_in.bias', 'control_model.middle_block.1.proj_in.weight', 'control_model.middle_block.1.proj_out.bias', 'control_model.middle_block.1.proj_out.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'control_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'control_model.middle_block.1.transformer_blocks.0.norm1.bias', 'control_model.middle_block.1.transformer_blocks.0.norm1.weight', 'control_model.middle_block.1.transformer_blocks.0.norm2.bias', 'control_model.middle_block.1.transformer_blocks.0.norm2.weight', 'control_model.middle_block.1.transformer_blocks.0.norm3.bias', 'control_model.middle_block.1.transformer_blocks.0.norm3.weight', 'control_model.middle_block.2.emb_layers.1.bias', 'control_model.middle_block.2.emb_layers.1.weight', 'control_model.middle_block.2.in_layers.0.bias', 'control_model.middle_block.2.in_layers.0.weight', 'control_model.middle_block.2.in_layers.2.bias', 'control_model.middle_block.2.in_layers.2.weight', 'control_model.middle_block.2.out_layers.0.bias', 'control_model.middle_block.2.out_layers.0.weight', 'control_model.middle_block.2.out_layers.3.bias', 'control_model.middle_block.2.out_layers.3.weight', 'control_model.middle_block_out.0.bias', 'control_model.middle_block_out.0.weight', 'control_model.time_embed.0.bias', 'control_model.time_embed.0.weight', 'control_model.time_embed.2.bias', 'control_model.time_embed.2.weight', 'control_model.zero_convs.0.0.bias', 'control_model.zero_convs.0.0.weight', 'control_model.zero_convs.1.0.bias', 'control_model.zero_convs.1.0.weight', 'control_model.zero_convs.10.0.bias', 'control_model.zero_convs.10.0.weight', 'control_model.zero_convs.11.0.bias', 'control_model.zero_convs.11.0.weight', 'control_model.zero_convs.2.0.bias', 'control_model.zero_convs.2.0.weight', 'control_model.zero_convs.3.0.bias', 'control_model.zero_convs.3.0.weight', 'control_model.zero_convs.4.0.bias', 'control_model.zero_convs.4.0.weight', 'control_model.zero_convs.5.0.bias', 'control_model.zero_convs.5.0.weight', 'control_model.zero_convs.6.0.bias', 'control_model.zero_convs.6.0.weight', 'control_model.zero_convs.7.0.bias', 'control_model.zero_convs.7.0.weight', 'control_model.zero_convs.8.0.bias', 'control_model.zero_convs.8.0.weight', 'control_model.zero_convs.9.0.bias', 'control_model.zero_convs.9.0.weight', 'embedding_manager.embedder.transformer.text_model.embeddings.position_embedding.weight', 'embedding_manager.embedder.transformer.text_model.embeddings.position_ids', 'embedding_manager.embedder.transformer.text_model.embeddings.token_embedding.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.final_layer_norm.bias', 'embedding_manager.embedder.transformer.text_model.final_layer_norm.weight', 'lora_te_text_model_encoder_layers_0_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_0_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_0_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_0_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_0_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_0_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_0_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_0_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_0_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_0_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_10_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_10_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_10_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_10_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_10_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_10_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_10_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_10_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_10_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_10_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_11_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_11_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_11_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_11_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_11_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_11_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_11_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_11_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_11_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_11_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_1_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_1_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_1_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_1_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_1_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_1_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_1_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_1_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_1_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_1_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_2_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_2_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_2_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_2_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_2_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_2_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_2_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_2_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_2_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_2_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_3_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_3_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_3_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_3_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_3_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_3_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_3_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_3_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_3_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_3_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_4_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_4_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_4_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_4_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_4_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_4_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_4_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_4_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_4_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_4_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_5_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_5_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_5_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_5_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_5_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_5_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_5_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_5_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_5_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_5_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_6_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_6_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_6_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_6_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_6_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_6_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_6_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_6_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_6_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_6_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_7_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_7_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_7_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_7_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_7_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_7_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_7_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_7_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_7_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_7_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_8_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_8_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_8_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_8_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_8_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_8_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_8_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_8_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_8_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_8_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_9_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_9_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_9_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_9_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_9_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_9_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_9_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_9_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_9_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_9_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_up.weight'])\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Global Step: 300167\n",
      "loading new\n",
      "loading new\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]Blocksparse is not available: the current GPU does not expose Tensor cores\n",
      "/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:594: UserWarning: Should have tb<=t1 but got tb=14.614643096923828 and t1=14.614643.\n",
      "  warnings.warn(f\"Should have {tb_name}<=t1 but got {tb_name}={tb} and t1={self._end}.\")\n",
      " 94%|#########4| 33/35 [00:37<00:02,  1.11s/it]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:585: UserWarning: Should have ta>=t0 but got ta=0.02916753850877285 and t0=0.029168.\n",
      "  warnings.warn(f\"Should have ta>=t0 but got ta={ta} and t0={self._start}.\")\n",
      "100%|##########| 35/35 [00:39<00:00,  1.13s/it]\n",
      "IterativeLatentUpscale[1/3]: 896.0x896.0 (scale:1.17) \n",
      " 93%|#########3| 28/30 [01:40<00:07,  3.57s/it]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:585: UserWarning: Should have ta>=t0 but got ta=0.0291675366461277 and t0=0.029168.\n",
      "  warnings.warn(f\"Should have ta>=t0 but got ta={ta} and t0={self._start}.\")\n",
      "100%|##########| 30/30 [01:45<00:00,  3.52s/it]\n",
      "IterativeLatentUpscale[2/3]: 1024.0x1024.0 (scale:1.33) \n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:594: UserWarning: Should have tb<=t1 but got tb=0.6401465535163879 and t1=0.640146.\n",
      "  warnings.warn(f\"Should have {tb_name}<=t1 but got {tb_name}={tb} and t1={self._end}.\")\n",
      "100%|##########| 30/30 [02:23<00:00,  4.77s/it]\n",
      "IterativeLatentUpscale[Final]: 1152.0x1152.0 (scale:1.50) \n",
      " 10%|#         | 3/30 [00:22<03:23,  7.53s/it]"
     ]
    }
   ],
   "source": [
    "run_option = 'cloudflared (Recommended Way)'\n",
    "print('Running ComfyUI with ', run_option)\n",
    "if run_option==\"cloudflared (Recommended Way)\":\n",
    "  !wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
    "  !dpkg -i cloudflared-linux-amd64.deb\n",
    "\n",
    "  import subprocess\n",
    "  import threading\n",
    "  import time\n",
    "  import socket\n",
    "  import urllib.request\n",
    "\n",
    "  def iframe_thread(port):\n",
    "    while True:\n",
    "        time.sleep(0.5)\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        result = sock.connect_ex(('127.0.0.1', port))\n",
    "        if result == 0:\n",
    "          break\n",
    "        sock.close()\n",
    "    print(\"\\nComfyUI finished loading, trying to launch cloudflared (if it gets stuck here cloudflared is having issues)\\n\")\n",
    "\n",
    "    p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    for line in p.stderr:\n",
    "      l = line.decode()\n",
    "      if \"trycloudflare.com \" in l:\n",
    "        print(\"This is the URL to access ComfyUI:\", l[l.find(\"http\"):], end='')\n",
    "      #print(l, end='')\n",
    "\n",
    "\n",
    "  threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
    "\n",
    "  !python main.py --dont-print-server\n",
    "\n",
    "\n",
    "if run_option==\"localtunnel\":\n",
    "  !npm install -g localtunnel\n",
    "\n",
    "  import subprocess\n",
    "  import threading\n",
    "  import time\n",
    "  import socket\n",
    "  import urllib.request\n",
    "\n",
    "  def iframe_thread(port):\n",
    "    while True:\n",
    "        time.sleep(0.5)\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        result = sock.connect_ex(('127.0.0.1', port))\n",
    "        if result == 0:\n",
    "          break\n",
    "        sock.close()\n",
    "    print(\"\\nComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\\n\")\n",
    "\n",
    "    print(\"The password/enpoint ip for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
    "    p = subprocess.Popen([\"lt\", \"--port\", \"{}\".format(port)], stdout=subprocess.PIPE)\n",
    "    for line in p.stdout:\n",
    "      print(line.decode(), end='')\n",
    "\n",
    "\n",
    "  threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
    "\n",
    "  !python main.py --dont-print-server"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
